{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from opacus import PrivacyEngine\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE MODEL (No DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8510594738458389\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      7414\n",
      "           1       0.74      0.59      0.65      2355\n",
      "\n",
      "    accuracy                           0.85      9769\n",
      "   macro avg       0.81      0.76      0.78      9769\n",
      "weighted avg       0.84      0.85      0.84      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = y.iloc[:, 0]\n",
    "# Strip spaces and remove periods\n",
    "y = y.str.strip().str.replace('.', '', regex=False)\n",
    "\n",
    "# Map both <=50K and <50K to 0; >=50K and >50K to 1\n",
    "y = y.replace({\n",
    "    '<=50K': 0,\n",
    "    '<50K': 0,\n",
    "    '>=50K': 1,\n",
    "    '>50K': 1\n",
    "})\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics(X_test, y_true, y_pred, feature):\n",
    "    df_eval = X_test.copy()\n",
    "    df_eval['y_true'] = y_true\n",
    "    df_eval['y_pred'] = y_pred\n",
    "    \n",
    "    groups = df_eval[feature].unique()\n",
    "    metrics = []\n",
    "\n",
    "    for g in groups:\n",
    "        mask = df_eval[feature] == g\n",
    "        y_t = df_eval.loc[mask, 'y_true']\n",
    "        y_p = df_eval.loc[mask, 'y_pred']\n",
    "\n",
    "        # Positive prediction rate (Demographic Parity)\n",
    "        pos_rate = np.mean(y_p)\n",
    "\n",
    "        # True Positive Rate (Equal Opportunity)\n",
    "        tp = np.sum((y_p == 1) & (y_t == 1))\n",
    "        fn = np.sum((y_p == 0) & (y_t == 1))\n",
    "        tpr = tp / (tp + fn + 1e-10)\n",
    "\n",
    "        # False Positive Rate (Equalized Odds)\n",
    "        fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "        tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "        fpr = fp / (fp + tn + 1e-10)\n",
    "\n",
    "        # Accuracy\n",
    "        acc = np.mean(y_p == y_t)\n",
    "\n",
    "        metrics.append({\n",
    "            'group': g,\n",
    "            'positive_rate': pos_rate,\n",
    "            'TPR': tpr,\n",
    "            'FPR': fpr,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    print(f\"\\n=== Fairness by {feature} ===\")\n",
    "    print(df_metrics)\n",
    "\n",
    "    # Group gaps\n",
    "    max_min_gap = df_metrics[['positive_rate', 'TPR', 'FPR', 'accuracy']].max() - df_metrics[['positive_rate', 'TPR', 'FPR', 'accuracy']].min()\n",
    "    print(\"\\nGaps between groups:\")\n",
    "    print(max_min_gap)\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation for No DP Logistic Regression ===\n",
      "Accuracy: 0.851\n",
      "AUC: 0.906\n",
      "\n",
      "=== Fairness by sex ===\n",
      "    group  positive_rate       TPR       FPR  accuracy\n",
      "0    Male       0.248623  0.600503  0.094589  0.812576\n",
      "1  Female       0.072688  0.506849  0.017434  0.928859\n",
      "\n",
      "Gaps between groups:\n",
      "positive_rate    0.175935\n",
      "TPR              0.093653\n",
      "FPR              0.077155\n",
      "accuracy         0.116282\n",
      "dtype: float64\n",
      "\n",
      "=== Fairness by race ===\n",
      "                group  positive_rate       TPR       FPR  accuracy\n",
      "0               White       0.203071  0.594747  0.068493  0.845388\n",
      "1               Black       0.077813  0.421488  0.027711  0.902208\n",
      "2  Amer-Indian-Eskimo       0.052083  0.375000  0.022727  0.927083\n",
      "3  Asian-Pac-Islander       0.264151  0.651163  0.120690  0.817610\n",
      "4               Other       0.059701  0.250000  0.033898  0.880597\n",
      "\n",
      "Gaps between groups:\n",
      "positive_rate    0.212068\n",
      "TPR              0.401163\n",
      "FPR              0.097962\n",
      "accuracy         0.109473\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_full(name, model, X_model_input, y_true, X_sensitive, fairness_metrics_func=fairness_metrics):\n",
    "    \"\"\"\n",
    "    Evaluates a model (PyTorch DP/non-DP or sklearn) with standard metrics and fairness metrics.\n",
    "\n",
    "    Args:\n",
    "        name (str): Model name (for printing)\n",
    "        model: sklearn model or PyTorch nn.Module\n",
    "        X_model_input: np.ndarray or torch.Tensor input for the model\n",
    "        y_true: array-like, true labels\n",
    "        X_sensitive: pd.DataFrame with sensitive features (e.g., 'sex', 'race') for fairness\n",
    "        fairness_metrics_func: function to compute fairness metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Detect if PyTorch model\n",
    "    is_torch_model = isinstance(model, torch.nn.Module)\n",
    "\n",
    "    if is_torch_model:\n",
    "        # Ensure tensor input\n",
    "        if not torch.is_tensor(X_model_input):\n",
    "            X_model_input = torch.tensor(X_model_input, dtype=torch.float32)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_model_input)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            y_pred = np.argmax(outputs.cpu().numpy(), axis=1)\n",
    "    else:\n",
    "        # sklearn model\n",
    "        y_pred = model.predict(X_model_input)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_model_input)[:, 1]\n",
    "        else:\n",
    "            probs = y_pred  # fallback if predict_proba not available\n",
    "\n",
    "    # Standard metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    print(f\"\\n=== Evaluation for {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "\n",
    "    # Fairness metrics\n",
    "    sex_metrics = race_metrics = None\n",
    "    if fairness_metrics_func and X_sensitive is not None:\n",
    "        if 'sex' in X_sensitive.columns:\n",
    "            sex_metrics = fairness_metrics_func(X_sensitive, y_true, y_pred, 'sex')\n",
    "        if 'race' in X_sensitive.columns:\n",
    "            race_metrics = fairness_metrics_func(X_sensitive, y_true, y_pred, 'race')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"auc\": auc,\n",
    "        \"sex_metrics\": sex_metrics,\n",
    "        \"race_metrics\": race_metrics\n",
    "    }\n",
    "\n",
    "# === Example usage ===\n",
    "\n",
    "# For non-DP model\n",
    "results_no_dp = evaluate_model_full(\n",
    "    name=\"No DP Logistic Regression\",\n",
    "    model=clf,                   # sklearn or PyTorch model\n",
    "    X_model_input=X_test_tensor if isinstance(clf, torch.nn.Module) else X_test, \n",
    "    y_true=y_test,\n",
    "    X_sensitive=X_test           # original DataFrame with sensitive features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model predicts that men are more likely to earn over $50K than women, which could reflect real income patterns but could also show bias.\n",
    "- It’s better at correctly identifying men who earn over $50K than women who do.\n",
    "- The model falsely labels men as rich more often than women.\n",
    "- Interestingly, it’s more accurate overall for women, possibly because fewer women are predicted as high earners, reducing some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enricoalmadani/anaconda3/lib/python3.10/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n",
      "DP-SGD Logistic Regression Test Accuracy: 0.8426\n"
     ]
    }
   ],
   "source": [
    "# Preprocess features with your existing preprocessor\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors (handle sparse output from OneHotEncoder if any)\n",
    "def to_tensor(x):\n",
    "    if hasattr(x, \"toarray\"):\n",
    "        return torch.tensor(x.toarray(), dtype=torch.float32)\n",
    "    return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "X_train_tensor = to_tensor(X_train_processed)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = to_tensor(X_test_processed)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LogisticRegressionModel(X_train_tensor.shape[1])\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) # we can tune the learning rate (lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup Privacy Engine for DP-SGD\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=2.0,  # we can tune this value (higher means high accuracy)\n",
    "    max_grad_norm=0.5, # we can tune this value (lower means high accuracy)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} done\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    preds = outputs.argmax(dim=1).numpy()\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"DP-SGD Logistic Regression Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2374250128408337"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation for DP-SGD Logistic Regression ===\n",
      "Accuracy: 0.843\n",
      "AUC: 0.896\n",
      "\n",
      "=== Fairness by sex ===\n",
      "    group  positive_rate       TPR       FPR  accuracy\n",
      "0    Male       0.236077  0.566332  0.091509  0.804315\n",
      "1  Female       0.057532  0.400000  0.013947  0.919889\n",
      "\n",
      "Gaps between groups:\n",
      "positive_rate    0.178545\n",
      "TPR              0.166332\n",
      "FPR              0.077562\n",
      "accuracy         0.115574\n",
      "dtype: float64\n",
      "\n",
      "=== Fairness by race ===\n",
      "                group  positive_rate       TPR       FPR  accuracy\n",
      "0               White       0.186878  0.544559  0.063981  0.835912\n",
      "1               Black       0.075710  0.413223  0.026506  0.902208\n",
      "2  Amer-Indian-Eskimo       0.041667  0.375000  0.011364  0.937500\n",
      "3  Asian-Pac-Islander       0.289308  0.662791  0.150862  0.798742\n",
      "4               Other       0.044776  0.250000  0.016949  0.895522\n",
      "\n",
      "Gaps between groups:\n",
      "positive_rate    0.247642\n",
      "TPR              0.412791\n",
      "FPR              0.139498\n",
      "accuracy         0.138758\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_dp = evaluate_model_full(\n",
    "    name=\"DP-SGD Logistic Regression\",\n",
    "    model=model,                 # PyTorch DP model\n",
    "    X_model_input=X_test_tensor, # tensor for model input\n",
    "    y_true=y_test,\n",
    "    X_sensitive=X_test           # original DataFrame for fairness\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

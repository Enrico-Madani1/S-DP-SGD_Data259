{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.accountants.analysis import rdp as privacy_analysis\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_random_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[:, 0]\n",
    "# Strip spaces and remove periods\n",
    "y = y.str.strip().str.replace('.', '', regex=False)\n",
    "\n",
    "# Map both <=50K and <50K to 0; >=50K and >50K to 1\n",
    "y = y.replace({\n",
    "    '<=50K': 0,\n",
    "    '<50K': 0,\n",
    "    '>=50K': 1,\n",
    "    '>50K': 1\n",
    "})\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics(X_test, y_true, y_pred, feature):\n",
    "    \"\"\"\n",
    "    Calculate fairness metrics for a specific sensitive feature.\n",
    "    \n",
    "    Returns demographic parity, equal opportunity (TPR), and equalized odds (FPR)\n",
    "    for each group in the feature.\n",
    "    \"\"\"\n",
    "    df_eval = X_test.copy()\n",
    "    df_eval['y_true'] = y_true.values if hasattr(y_true, 'values') else y_true\n",
    "    df_eval['y_pred'] = y_pred\n",
    "    \n",
    "    groups = df_eval[feature].unique()\n",
    "    metrics = []\n",
    "\n",
    "    for g in groups:\n",
    "        mask = df_eval[feature] == g\n",
    "        y_t = df_eval.loc[mask, 'y_true']\n",
    "        y_p = df_eval.loc[mask, 'y_pred']\n",
    "\n",
    "        # Positive prediction rate (Demographic Parity)\n",
    "        pos_rate = np.mean(y_p)\n",
    "\n",
    "        # True Positive Rate (Equal Opportunity)\n",
    "        tp = np.sum((y_p == 1) & (y_t == 1))\n",
    "        fn = np.sum((y_p == 0) & (y_t == 1))\n",
    "        tpr = tp / (tp + fn + 1e-10)\n",
    "\n",
    "        # False Positive Rate (Equalized Odds)\n",
    "        fp = np.sum((y_p == 1) & (y_t == 0))\n",
    "        tn = np.sum((y_p == 0) & (y_t == 0))\n",
    "        fpr = fp / (fp + tn + 1e-10)\n",
    "\n",
    "        # Accuracy\n",
    "        acc = np.mean(y_p == y_t)\n",
    "\n",
    "        metrics.append({\n",
    "            'group': g,\n",
    "            'positive_rate': pos_rate,\n",
    "            'TPR': tpr,\n",
    "            'FPR': fpr,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    print(f\"\\n=== Fairness by {feature} ===\")\n",
    "    print(df_metrics.to_string(index=False))\n",
    "\n",
    "    # Calculate gaps between groups\n",
    "    max_min_gap = df_metrics[['positive_rate', 'TPR', 'FPR', 'accuracy']].max() - \\\n",
    "                  df_metrics[['positive_rate', 'TPR', 'FPR', 'accuracy']].min()\n",
    "    print(\"\\nGaps between groups (max - min):\")\n",
    "    print(max_min_gap.to_string())\n",
    "\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def evaluate_model_full(name, model, X_model_input, y_true, X_sensitive, \n",
    "                        fairness_metrics_func=fairness_metrics):\n",
    "    \"\"\"\n",
    "    Evaluate a model with standard metrics and fairness metrics.\n",
    "    \n",
    "    Works with both sklearn models and PyTorch models.\n",
    "    \"\"\"\n",
    "    # Detect if PyTorch model\n",
    "    is_torch_model = isinstance(model, torch.nn.Module)\n",
    "\n",
    "    if is_torch_model:\n",
    "        # Ensure tensor input\n",
    "        if not torch.is_tensor(X_model_input):\n",
    "            X_model_input = torch.tensor(X_model_input, dtype=torch.float32)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_model_input)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            y_pred = np.argmax(outputs.cpu().numpy(), axis=1)\n",
    "    else:\n",
    "        # sklearn model\n",
    "        y_pred = model.predict(X_model_input)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_model_input)[:, 1]\n",
    "        else:\n",
    "            probs = y_pred\n",
    "\n",
    "    # Standard metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "    print(f\"AUC-ROC Score: {auc:.4f}\")\n",
    "\n",
    "    # Fairness metrics\n",
    "    sex_metrics = race_metrics = None\n",
    "    if fairness_metrics_func and X_sensitive is not None:\n",
    "        if 'sex' in X_sensitive.columns:\n",
    "            sex_metrics = fairness_metrics_func(X_sensitive, y_true, y_pred, 'sex')\n",
    "        if 'race' in X_sensitive.columns:\n",
    "            race_metrics = fairness_metrics_func(X_sensitive, y_true, y_pred, 'race')\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"auc\": auc,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"sex_metrics\": sex_metrics,\n",
    "        \"race_metrics\": race_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    \"\"\"Simple logistic regression model for binary classification.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL (NO DIFFERENTIAL PRIVACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_model(X_train, y_train, X_test, y_test, \n",
    "                         max_iter=1000, random_state=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Train a baseline logistic regression model without any privacy protection.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data (pandas DataFrames/Series)\n",
    "        X_test, y_test: Test data\n",
    "        max_iter: Maximum iterations for sklearn LogisticRegression\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained sklearn pipeline\n",
    "        results: Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING BASELINE MODEL (No Differential Privacy)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create preprocessing + model pipeline\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=max_iter, random_state=random_state))\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"✓ Training completed\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model_full(\n",
    "        name=\"Baseline (No DP)\",\n",
    "        model=clf,\n",
    "        X_model_input=X_test,\n",
    "        y_true=y_test,\n",
    "        X_sensitive=X_test\n",
    "    )\n",
    "    \n",
    "    return clf, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARD DP-SGD MODEL (OPACUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_standard_dpsgd(X_train_tensor, y_train_tensor, X_test_tensor, y_test,\n",
    "                         X_test_df, noise_multiplier=1.0, max_grad_norm=1.0,\n",
    "                         lr=0.01, epochs=10, batch_size=64, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model with standard DP-SGD using Opacus.\n",
    "    All gradients are protected with differential privacy.\n",
    "    \n",
    "    Args:\n",
    "        X_train_tensor: Preprocessed training features (torch tensor)\n",
    "        y_train_tensor: Training labels (torch tensor)\n",
    "        X_test_tensor: Preprocessed test features (torch tensor)\n",
    "        y_test: Test labels (pandas Series or numpy array)\n",
    "        X_test_df: Original test dataframe for fairness evaluation\n",
    "        noise_multiplier: Controls noise magnitude (higher = more privacy)\n",
    "        max_grad_norm: Gradient clipping threshold (lower = more privacy)\n",
    "        lr: Learning rate\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        delta: Privacy parameter (typically 1e-5)\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained PyTorch model\n",
    "        epsilon: Privacy budget (ε)\n",
    "        results: Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING STANDARD DP-SGD MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Hyperparameters:\")\n",
    "    print(f\"  - Learning rate: {lr}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Noise multiplier: {noise_multiplier}\")\n",
    "    print(f\"  - Max grad norm: {max_grad_norm}\")\n",
    "    print(f\"  - Delta (δ): {delta}\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LogisticRegressionModel(X_train_tensor.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Attach Privacy Engine\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training completed\")\n",
    "    \n",
    "    # Calculate privacy budget\n",
    "    epsilon = privacy_engine.get_epsilon(delta=delta)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PRIVACY BUDGET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Epsilon (ε): {epsilon:.4f}\")\n",
    "    print(f\"Delta (δ): {delta}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  - ALL features are protected with ε={epsilon:.4f} privacy\")\n",
    "    print(f\"  - Lower ε = stronger privacy (typical target: ε ≤ 1.0)\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model_full(\n",
    "        name=f\"Standard DP-SGD (ε={epsilon:.3f})\",\n",
    "        model=model,\n",
    "        X_model_input=X_test_tensor,\n",
    "        y_true=y_test,\n",
    "        X_sensitive=X_test_df\n",
    "    )\n",
    "    \n",
    "    results['epsilon'] = epsilon\n",
    "    results['delta'] = delta\n",
    "    \n",
    "    return model, epsilon, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECTIVE DP-SGD MODEL (MANUAL IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selective_dpsgd(X_train_tensor, y_train_tensor, X_test_tensor, y_test,\n",
    "                          X_test_df, sensitive_indices, noise_multiplier=1.0,\n",
    "                          max_grad_norm=1.0, lr=0.01, epochs=10, batch_size=64,\n",
    "                          delta=1e-5):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model with Selective DP-SGD.\n",
    "    Only gradients corresponding to sensitive features are protected with DP noise.\n",
    "    \n",
    "    Args:\n",
    "        X_train_tensor: Preprocessed training features (torch tensor)\n",
    "        y_train_tensor: Training labels (torch tensor)\n",
    "        X_test_tensor: Preprocessed test features (torch tensor)\n",
    "        y_test: Test labels\n",
    "        X_test_df: Original test dataframe for fairness evaluation\n",
    "        sensitive_indices: List of feature indices to protect (e.g., sex, race)\n",
    "        noise_multiplier: Controls noise magnitude (higher = more privacy)\n",
    "        max_grad_norm: Gradient clipping threshold (lower = more privacy)\n",
    "        lr: Learning rate\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        delta: Privacy parameter\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained PyTorch model\n",
    "        epsilon: Privacy budget for sensitive features\n",
    "        results: Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING SELECTIVE DP-SGD MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Hyperparameters:\")\n",
    "    print(f\"  - Learning rate: {lr}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Noise multiplier: {noise_multiplier}\")\n",
    "    print(f\"  - Max grad norm: {max_grad_norm}\")\n",
    "    print(f\"  - Delta (δ): {delta}\")\n",
    "    print(f\"  - Sensitive feature indices: {sensitive_indices}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LogisticRegressionModel(X_train_tensor.shape[1])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop with selective DP\n",
    "    n_samples = len(X_train_tensor)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        indices = torch.randperm(n_samples)\n",
    "        X_shuffled = X_train_tensor[indices]\n",
    "        y_shuffled = y_train_tensor[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply selective DP to gradients\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if \"linear.weight\" in name and param.grad is not None:\n",
    "                        grad = param.grad\n",
    "                        \n",
    "                        # Create mask for sensitive features\n",
    "                        mask = torch.zeros_like(grad)\n",
    "                        mask[:, sensitive_indices] = 1.0\n",
    "                        \n",
    "                        # Clip sensitive feature gradients\n",
    "                        sensitive_grad = grad * mask\n",
    "                        grad_norm = sensitive_grad.norm(2)\n",
    "                        if grad_norm > max_grad_norm:\n",
    "                            # Normalize only sensitive gradients\n",
    "                            clipped_sensitive = sensitive_grad * (max_grad_norm / grad_norm)\n",
    "                            # Combine: non-sensitive (unchanged) + clipped sensitive\n",
    "                            grad.data = grad * (1.0 - mask) + clipped_sensitive\n",
    "                        \n",
    "                        # Add Gaussian noise ONLY to sensitive features\n",
    "                        noise = torch.normal(\n",
    "                            mean=0.0,\n",
    "                            std=noise_multiplier * max_grad_norm,\n",
    "                            size=grad.shape,\n",
    "                            device=grad.device\n",
    "                        )\n",
    "                        param.grad.add_(noise * mask)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training completed\")\n",
    "    \n",
    "    # Calculate privacy budget for sensitive features\n",
    "    steps = epochs * n_batches\n",
    "    q = batch_size / n_samples  # Sampling rate\n",
    "    \n",
    "    # Use RDP accountant to compute APPROXIMATE epsilon\n",
    "    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "    rdp = privacy_analysis.compute_rdp(\n",
    "        q=q,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        steps=steps,\n",
    "        orders=orders\n",
    "    )\n",
    "    epsilon = privacy_analysis.get_privacy_spent(orders=orders, rdp=rdp, delta=delta)[0]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PRIVACY BUDGET (SELECTIVE)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Epsilon (ε): {epsilon:.4f}\")\n",
    "    print(f\"Delta (δ): {delta}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  - Sensitive features (indices {sensitive_indices}) have ε={epsilon:.4f} privacy\")\n",
    "    print(f\"  - Non-sensitive features have NO formal privacy guarantee\")\n",
    "    print(f\"  - Lower ε = stronger privacy (typical target: ε ≤ 1.0)\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model_full(\n",
    "        name=f\"Selective DP-SGD (ε={epsilon:.3f})\",\n",
    "        model=model,\n",
    "        X_model_input=X_test_tensor,\n",
    "        y_true=y_test,\n",
    "        X_sensitive=X_test_df\n",
    "    )\n",
    "    \n",
    "    results['epsilon'] = epsilon\n",
    "    results['delta'] = delta\n",
    "    \n",
    "    return model, epsilon, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING DATA FOR PYTORCH MODELS\n",
      "======================================================================\n",
      "✓ Train tensor shape: torch.Size([39073, 111])\n",
      "✓ Test tensor shape: torch.Size([9769, 111])\n",
      "\n",
      "======================================================================\n",
      "IDENTIFYING SENSITIVE FEATURES\n",
      "======================================================================\n",
      "Total features after encoding: 111\n",
      "Sensitive feature indices: [61, 62, 63, 64, 65, 66, 67]\n",
      "Sensitive feature names:\n",
      "  [61] cat__race_Amer-Indian-Eskimo\n",
      "  [62] cat__race_Asian-Pac-Islander\n",
      "  [63] cat__race_Black\n",
      "  [64] cat__race_Other\n",
      "  [65] cat__race_White\n",
      "  [66] cat__sex_Female\n",
      "  [67] cat__sex_Male\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING DATA FOR PYTORCH MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    def to_tensor(x):\n",
    "        if hasattr(x, \"toarray\"):\n",
    "            return torch.tensor(x.toarray(), dtype=torch.float32)\n",
    "        return torch.tensor(x, dtype=torch.float32)\n",
    "    \n",
    "    X_train_tensor = to_tensor(X_train_processed)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = to_tensor(X_test_processed)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Train tensor shape: {X_train_tensor.shape}\")\n",
    "    print(f\"Test tensor shape: {X_test_tensor.shape}\")\n",
    "    \n",
    "    # Identify sensitive feature indices\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"IDENTIFYING SENSITIVE FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    encoded_feature_names = preprocessor.get_feature_names_out()\n",
    "    sensitive_indices = []\n",
    "    for idx, name in enumerate(encoded_feature_names):\n",
    "        if name.startswith('cat__sex_') or name.startswith('cat__race_'):\n",
    "            sensitive_indices.append(idx)\n",
    "    \n",
    "    print(f\"Total features after encoding: {len(encoded_feature_names)}\")\n",
    "    print(f\"Sensitive feature indices: {sensitive_indices}\")\n",
    "    print(f\"Sensitive feature names:\")\n",
    "    for idx in sensitive_indices:\n",
    "        print(f\"  [{idx}] {encoded_feature_names[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING BASELINE MODEL (No Differential Privacy)\n",
      "======================================================================\n",
      "✓ Training completed\n",
      "\n",
      "======================================================================\n",
      "EVALUATION: Baseline (No DP)\n",
      "======================================================================\n",
      "Overall Accuracy: 0.8517\n",
      "AUC-ROC Score: 0.9058\n",
      "\n",
      "=== Fairness by sex ===\n",
      " group  positive_rate      TPR      FPR  accuracy\n",
      "  Male       0.256173 0.617453 0.098248  0.815278\n",
      "Female       0.075707 0.495913 0.022930  0.923381\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.180466\n",
      "TPR              0.121540\n",
      "FPR              0.075318\n",
      "accuracy         0.108103\n",
      "\n",
      "=== Fairness by race ===\n",
      "             group  positive_rate      TPR      FPR  accuracy\n",
      "             White       0.209391 0.606890 0.074169  0.844873\n",
      "             Other       0.072464 0.363636 0.017241  0.884058\n",
      "             Black       0.072034 0.421488 0.020656  0.907839\n",
      "Asian-Pac-Islander       0.266447 0.679487 0.123894  0.825658\n",
      "Amer-Indian-Eskimo       0.067308 0.555556 0.021053  0.942308\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.199140\n",
      "TPR              0.315851\n",
      "FPR              0.106652\n",
      "accuracy         0.116650\n"
     ]
    }
   ],
   "source": [
    "#1. Baseline Model\n",
    "baseline_model, baseline_results = train_baseline_model(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    max_iter=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING STANDARD DP-SGD MODEL\n",
      "======================================================================\n",
      "Hyperparameters:\n",
      "  - Learning rate: 0.01\n",
      "  - Epochs: 10\n",
      "  - Batch size: 64\n",
      "  - Noise multiplier: 1.0\n",
      "  - Max grad norm: 1.0\n",
      "  - Delta (δ): 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enricoalmadani/anaconda3/lib/python3.10/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.5353\n",
      "Epoch 2/10 - Loss: 0.4868\n",
      "Epoch 3/10 - Loss: 0.4368\n",
      "Epoch 4/10 - Loss: 0.4037\n",
      "Epoch 5/10 - Loss: 0.3907\n",
      "Epoch 6/10 - Loss: 0.3885\n",
      "Epoch 7/10 - Loss: 0.3817\n",
      "Epoch 8/10 - Loss: 0.3775\n",
      "Epoch 9/10 - Loss: 0.3798\n",
      "Epoch 10/10 - Loss: 0.3767\n",
      "✓ Training completed\n",
      "\n",
      "======================================================================\n",
      "PRIVACY BUDGET\n",
      "======================================================================\n",
      "Epsilon (ε): 0.6400\n",
      "Delta (δ): 1e-05\n",
      "\n",
      "Interpretation:\n",
      "  - ALL features are protected with ε=0.6400 privacy\n",
      "  - Lower ε = stronger privacy (typical target: ε ≤ 1.0)\n",
      "\n",
      "======================================================================\n",
      "EVALUATION: Standard DP-SGD (ε=0.640)\n",
      "======================================================================\n",
      "Overall Accuracy: 0.8338\n",
      "AUC-ROC Score: 0.8855\n",
      "\n",
      "=== Fairness by sex ===\n",
      " group  positive_rate      TPR      FPR  accuracy\n",
      "  Male       0.225309 0.535261 0.089820  0.796142\n",
      "Female       0.033445 0.237057 0.007871  0.907875\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.191864\n",
      "TPR              0.298204\n",
      "FPR              0.081949\n",
      "accuracy         0.111733\n",
      "\n",
      "=== Fairness by race ===\n",
      "             group  positive_rate      TPR      FPR  accuracy\n",
      "             White       0.167106 0.486550 0.058436  0.826066\n",
      "             Other       0.115942 0.636364 0.017241  0.927536\n",
      "             Black       0.063559 0.363636 0.019441  0.901483\n",
      "Asian-Pac-Islander       0.319079 0.717949 0.181416  0.792763\n",
      "Amer-Indian-Eskimo       0.096154 0.444444 0.063158  0.894231\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.255520\n",
      "TPR              0.354312\n",
      "FPR              0.164175\n",
      "accuracy         0.134773\n"
     ]
    }
   ],
   "source": [
    "# 2. Standard DP-SGD Model\n",
    "dpsgd_model, dpsgd_epsilon, dpsgd_results = train_standard_dpsgd(\n",
    "        X_train_tensor=X_train_tensor,\n",
    "        y_train_tensor=y_train_tensor,\n",
    "        X_test_tensor=X_test_tensor,\n",
    "        y_test=y_test,\n",
    "        X_test_df=X_test,\n",
    "        noise_multiplier=1.0,\n",
    "        max_grad_norm=1.0,\n",
    "        lr=0.01,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        delta=1e-5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING SELECTIVE DP-SGD MODEL\n",
      "======================================================================\n",
      "Hyperparameters:\n",
      "  - Learning rate: 0.01\n",
      "  - Epochs: 10\n",
      "  - Batch size: 64\n",
      "  - Noise multiplier: 1.0\n",
      "  - Max grad norm: 1.0\n",
      "  - Delta (δ): 1e-05\n",
      "  - Sensitive feature indices: [61, 62, 63, 64, 65, 66, 67]\n",
      "Epoch 1/10 - Loss: 0.4232\n",
      "Epoch 2/10 - Loss: 0.3587\n",
      "Epoch 3/10 - Loss: 0.3476\n",
      "Epoch 4/10 - Loss: 0.3418\n",
      "Epoch 5/10 - Loss: 0.3372\n",
      "Epoch 6/10 - Loss: 0.3332\n",
      "Epoch 7/10 - Loss: 0.3328\n",
      "Epoch 8/10 - Loss: 0.3310\n",
      "Epoch 9/10 - Loss: 0.3287\n",
      "Epoch 10/10 - Loss: 0.3294\n",
      "✓ Training completed\n",
      "\n",
      "======================================================================\n",
      "PRIVACY BUDGET (SELECTIVE)\n",
      "======================================================================\n",
      "Epsilon (ε): 0.9219\n",
      "Delta (δ): 1e-05\n",
      "\n",
      "Interpretation:\n",
      "  - Sensitive features (indices [61, 62, 63, 64, 65, 66, 67]) have ε=0.9219 privacy\n",
      "  - Non-sensitive features have NO formal privacy guarantee\n",
      "  - Lower ε = stronger privacy (typical target: ε ≤ 1.0)\n",
      "\n",
      "======================================================================\n",
      "EVALUATION: Selective DP-SGD (ε=0.922)\n",
      "======================================================================\n",
      "Overall Accuracy: 0.8450\n",
      "AUC-ROC Score: 0.8996\n",
      "\n",
      "=== Fairness by sex ===\n",
      " group  positive_rate      TPR      FPR  accuracy\n",
      "  Male       0.245525 0.589041 0.095365  0.808642\n",
      "Female       0.069018 0.435967 0.022930  0.916692\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.176507\n",
      "TPR              0.153074\n",
      "FPR              0.072435\n",
      "accuracy         0.108050\n",
      "\n",
      "=== Fairness by race ===\n",
      "             group  positive_rate      TPR      FPR  accuracy\n",
      "             White       0.190345 0.562529 0.063734  0.841399\n",
      "             Other       0.072464 0.363636 0.017241  0.884058\n",
      "             Black       0.100636 0.520661 0.038882  0.904661\n",
      "Asian-Pac-Islander       0.286184 0.705128 0.141593  0.819079\n",
      "Amer-Indian-Eskimo       0.403846 0.777778 0.368421  0.644231\n",
      "\n",
      "Gaps between groups (max - min):\n",
      "positive_rate    0.331382\n",
      "TPR              0.414141\n",
      "FPR              0.351180\n",
      "accuracy         0.260430\n"
     ]
    }
   ],
   "source": [
    "# 3. Selective DP-SGD Model\n",
    "selective_model, selective_epsilon, selective_results = train_selective_dpsgd(\n",
    "        X_train_tensor=X_train_tensor,\n",
    "        y_train_tensor=y_train_tensor,\n",
    "        X_test_tensor=X_test_tensor,\n",
    "        y_test=y_test,\n",
    "        X_test_df=X_test,\n",
    "        sensitive_indices=sensitive_indices,\n",
    "        noise_multiplier=1.0,\n",
    "        max_grad_norm=1.0,\n",
    "        lr=0.01,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        delta=1e-5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
